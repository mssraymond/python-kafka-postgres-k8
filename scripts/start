#!/bin/bash

RETRY_WAIT=10

wait_for_pod() {
    local pod_name="$1"
    while ! kubectl get pods | grep "$pod_name" | grep -E "Running|Completed" > /dev/null; do
        echo "Awaiting $pod_name..."
        sleep $RETRY_WAIT
    done
    echo "$pod_name ready!"
}

clear
: > log.txt

# Initialize cluster and namespace
kind create cluster --config manifests/0-kind-config.yml
kubectl create namespace kafka
kubectl config set-context --current --namespace kafka

# Provision Kafka and Schema Registry resources
kubectl apply -f manifests/1-kafka.yml
kubectl apply -f manifests/2-schema-registry.yml

# Await readiness...
wait_for_pod "kafka"
wait_for_pod "registry-schema"

# Build and load Docker images
docker build -t confluent-producer:1.0.0 -f producer/Dockerfile .
kind load docker-image confluent-producer:1.0.0
docker build -t kafka-connect-postgres:1.0.0 -f connect/Dockerfile .
kind load docker-image kafka-connect-postgres:1.0.0

# Provision Kafka Connect, PostgreSQL, and pgAdmin resources
kubectl apply -f manifests/3-connect.yml
kubectl apply -f manifests/4-postgres.yml
kubectl apply -f manifests/5-pgadmin.yml

wait_for_pod "kafka-connect"
wait_for_pod "postgres"
wait_for_pod "pgadmin"

# Port-forward necessary services
./scripts/port-forward $RETRY_WAIT &

# POST sink connector configurations
sleep 10s
until curl -X POST -H "Content-Type: application/json" --data @configs/post-sink-connector.json http://localhost:8083/connectors; do
    echo "Retry POST sink-connector in $RETRY_WAIT seconds..."
    sleep $RETRY_WAIT
done

# Provision producer resources
kubectl apply -f manifests/6-producer.yml

wait_for_pod "confluent-producer"

# Helm Kubeflow Spark Operator
helm repo add spark-operator https://kubeflow.github.io/spark-operator --force-update
helm repo update
helm install spark-operator spark-operator/spark-operator \
    --namespace spark-operator --create-namespace \
    --set "spark.jobNamespaces={kafka}" \
    --wait  # MUST HAVE `--set "spark.jobNamespaces={kafka}"`

# Give producers time to emit some events
sleep 10s

# Deploy Spark Job
docker build -t spark-pg:1.0.0 -f spark/Dockerfile .
kind load docker-image spark-pg:1.0.0
kubectl apply -f manifests/7-spark.yml
wait_for_pod "spark-python-driver"
kubectl logs spark-python-driver

echo "Suited and booted. Let's do it! ðŸš€"

### OTHER IMPORTANT COMMANDS ###
# Update connector configs: curl -X PUT -H "Content-Type: application/json" --data @configs/put-sink-connector.json http://localhost:8083/connectors/postgres-sink-connector/config
# Soft & hard delete registered schema: curl -X DELETE 'http://localhost:8081/subjects/loggings-value?permanent=false' && curl -X DELETE 'http://localhost:8081/subjects/loggings-value?permanent=true'